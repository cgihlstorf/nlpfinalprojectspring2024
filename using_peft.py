# -*- coding: utf-8 -*-
"""Using PEFT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCZJOGfiBoo8LBChajsOEQNjEdnMTgqu

* Huggingface's PEFT (Parameter-Efficient Fine-Tuning) library uses LORA
"""

#pip install transformers datasets evaluate accelerate peft 

import torch
from transformers import RobertaModel, RobertaTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from peft import LoraConfig, get_peft_model
from datasets import load_dataset, Dataset

peft_model_name = 'roberta-base-peft'
modified_base = 'roberta-base-modified'
base_model = 'roberta-base'

# useful if you don’t have enough space on your disk to download the dataset, or
# if you don’t want to wait for your dataset to be downloaded before using it
dataset_0 = load_dataset("oscar-corpus/OSCAR-2201",
                       use_auth_token=True,
                       language="fr",
                       #trust_remote_code=True,
                       streaming=True,
                       split="train"
                      )

#create a new dataset with fewer items than the original
#code from https://huggingface.co/docs/datasets/create_dataset
items_list = []
counter = 0
for item in dataset_0:
  if counter == 10:
    break
  items_list.append(item)
  counter += 1

#all code in the next two cells from https://huggingface.co/docs/datasets/create_dataset

def gen():
  for item in items_list:
    yield item

dataset = Dataset.from_generator(gen)

print(dataset.features)
print("VALS:", dataset.features['meta']['identification']['label'])

tokenizer = RobertaTokenizer.from_pretrained(base_model)

def preprocess(examples):
    tokenized = tokenizer(examples['text'], truncation=True, padding=True)
    return tokenized

train_dataset = dataset.map(preprocess, batched=True,  remove_columns=["text"])
# train_dataset=tokenized_dataset['train']
# eval_dataset=tokenized_dataset['test'].shard(num_shards=2, index=0)
# test_dataset=tokenized_dataset['test'].shard(num_shards=2, index=1)

# Extract the number of classess and their names
# num_labels = dataset.features['meta']['annotations'].num_classes
# class_names = dataset.features['meta']['annotations'].names
# print(f"number of labels: {num_labels}")
# print(f"the labels: {class_names}")

# Create an id2label mapping
# We will need this for our classifier.
# id2label = {i: label for i, label in enumerate(class_names)}

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

"""Training"""

# use the same Training args for all models
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy='steps',
    learning_rate=5e-5,
    num_train_epochs=1,
    per_device_train_batch_size=16,
)



def get_trainer(model):
      return  Trainer(
          model=model,
          args=training_args,
          train_dataset=train_dataset,
          #eval_dataset=eval_dataset,
          data_collator=data_collator,
      )

model = AutoModelForSequenceClassification.from_pretrained(base_model) #id2label=id2label)

peft_config = LoraConfig(task_type="SEQ_CLS", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)
peft_model = get_peft_model(model, peft_config)

print('PEFT Model')
peft_model.print_trainable_parameters()

peft_lora_finetuning_trainer = get_trainer(peft_model)

peft_lora_finetuning_trainer.train()

peft_lora_finetuning_trainer.save_model("finetuned_roberta_general_french")
#peft_lora_finetuning_trainer.evaluate()